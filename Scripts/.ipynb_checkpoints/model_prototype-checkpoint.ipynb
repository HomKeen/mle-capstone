{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb40958d-8840-4056-a118-1403206794c3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This prototype is a fully functional custom class that trains the RNN classifier on slice windows surrounding a given CT image slice (i.e., the nearby slices in the CT scan). It uses extracted feature vectors from a previously tuned CNN (using ResNet weights) which predicted intracranial hemorrhaging (ICH) on only single slices, instead of slice windows. The idea is to use nearby slices to make a more informed decision on the presence of ICH, while also using extracted features to drastically reduce computational demand. The output is a prediction of ICH for all slices in the window, and the network updates its weights based on the loss of every prediction in the slice. We may also use this network to predict on entire CT scans at once, however this is computational infeasible during training but may be helpful during testing. Given a directory of preprocessed data and some CSV files for metadata and labels, the model can be constructed and trained in a few lines. It may also be instantiated by a previously trained instance of this model to continue training. \n",
    "\n",
    "## Scalability\n",
    "As the training dataset alone contains 750,000+ DICOM images, each containing a 512x512 pixel image, I had to design this model to be able to handle a very large amount of input from the ground up. \n",
    "\n",
    "Since each DICOM image has a varying range of pixel values, I normalized each into [0,255], computed their window images, and saved it as a 512x512x3 RGB image in a PNG file, allowing for faster access than a DICOM file. I used ResNet-101 ImageNet weights from TensorFlow (to reduce training time) and tuned them to make a decent prediction on single DICOM images. This network is trained on a subset of the network until convergence, and it is not necessary to train it further since its only use beyond this point is a feature extractor. \n",
    "The RNN can be trained on a random subset of the data of desired size, or on the whole dataset, making it very scalable. The model will first compute all the feature vectors on the subset/set of data, store them in NPY files, and access them during training; this precomputation means that we can train much quicker than if we compute each feature vector as we go. Additionally, as the slice windows may overlap, we want to avoid re-computing features for the same image. The model is also designed to be trained on an NVIDIA GPU.\n",
    "\n",
    "All these implementation decisions contribute to a scalable model. I had to make a compromise for the sake of computational speed by using feature vectors from a tuned CNN, however when tuning this CNN it was clear that its predictions where only decent; this may have an impact on the quality of the feature vectors. I also require lots of storage space in order to store the feature vectors, which was another compromise for speed.\n",
    "\n",
    "## Todo for final product\n",
    "- Implement the use of validation sets during training\n",
    "- Rigorously examine the performance of the model: e.g., does using slice windows help a lot or a little? Which classes does the model fail on the most? How much worse is the model at predicting the ICH subtype, rather than simply detecting the presence?\n",
    "- Add some functionality to predict on test set in order to determine my score when uploaded to the Kaggle competition.\n",
    "- Examine performance when predicting on entire CT scans instead of some window (does it help?). In the real world, radiologists will have the entire CT scan at their disposal when searching for ICH, so it makes sense to allow our model to have the same information.\n",
    "    - Examine performance with smaller window sizes as well\n",
    "- Convert model into a deployable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedc4ab2-54e2-4a13-8919-e5239dad5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dcm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d60781-b8f4-4912-bda4-5741b2121ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "train_img_dir = 'rsna-intracranial-hemorrhage-detection/stage_2_train_imgs/'\n",
    "train_label_path = 'rsna-intracranial-hemorrhage-detection/train_labels.csv'\n",
    "train_ct_path = 'rsna-intracranial-hemorrhage-detection/train_ct_scans.csv'\n",
    "train_coord_path = 'rsna-intracranial-hemorrhage-detection/train_ct_coords.csv'\n",
    "base_model_path = '/home/jupyter/base-cnn-model/checkpoint.ckpt'\n",
    "\n",
    "saved_model_path = 'experiment-3-checkpoints/checkpoint.ckpt'\n",
    "\n",
    "test_img_dir = 'rsna-intracranial-hemorrhage-detection/stage_2_test_imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48720c46-bb9c-4c65-b7f6-7d1bfd567787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_tensor(img_path):\n",
    "    return tf.convert_to_tensor(np.asarray(Image.open(img_path), dtype=np.float32) / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9922a30b-d364-4d8b-9a05-a88a5b49bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNASequence(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A keras Sequence which provides training data to the model\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, train_cutoff, batch_size, extractor, img_dir, n_slices, \\\n",
    "                train_img_ct, train_img_ct_ind, feature_dir='extracted_features/', ):\n",
    "        self.x = None\n",
    "        self.y = labels #DataFrame of all labels for the whole training set\n",
    "        self.train_cutoff = train_cutoff #number of images to predict for in one epoch\n",
    "        self.batch_size = batch_size #number of image clusters to train on in one batch\n",
    "        self.img_dir = img_dir #directory containing PNG images for training\n",
    "        self.n_slices = n_slices #number of slices both above and below, to collect in a single training data point\n",
    "        self.extractor = extractor #extractor model\n",
    "        self.feature_dir = feature_dir #directory containing .npy files which are feature vectors of each image. The directory may be incomplete;\n",
    "                                        #if so, this class will automatically generate feature vectors as needed\n",
    "        self.train_img_ct = train_img_ct\n",
    "        self.train_img_ct_ind = train_img_ct_ind\n",
    "        \n",
    "        self.on_epoch_end() #initialize a random subset of data\n",
    "    \n",
    "    def get_nearby_slices_names(self, img_id, n_slices):\n",
    "        \"\"\"\n",
    "        :img_id (str): the ID of the slice to find nearby slices for\n",
    "        :n_slices (int): number of slices, both above and below, to return.\n",
    "        \n",
    "        :return: list of 2*n_slices + 1 IDs of nearby images\n",
    "        \n",
    "        Gets a list of names of the nearby image slices, rather than their feature vectors or raw image values\n",
    "        \"\"\"\n",
    "        ct_ind, ind = self.train_img_ct_ind[img_id]['ct_ind'], self.train_img_ct_ind[img_id]['ind']\n",
    "        ct = self.train_img_ct[ct_ind]\n",
    "        n = len(ct)\n",
    "        low, high = ind-n_slices, ind+n_slices\n",
    "        if low < 0:\n",
    "            high += abs(low)\n",
    "            low = 0\n",
    "        elif high >= n:\n",
    "            low -= high - (n-1)\n",
    "            high = n-1\n",
    "        return ct[low:high+1]\n",
    "    \n",
    "    def get_nearby_slices(self, img_id, n_slices):\n",
    "        '''\n",
    "        :img_id (str): the ID of the slice to find nearby slices for\n",
    "        :n_slices (int): number of slices, both above and below, to return.\n",
    "        \n",
    "        :return: list of 2*n_slices + 1 images as TensorFlow tensor\n",
    "        \n",
    "        Will retrieve n_slices slices from BOTH above and below the given image. If there is not enough space, it will\n",
    "        add more slices either below (if the image is near the top of the scan) or above (if the image is near the bottom of the scan).\n",
    "        Exactly 2*n_slices + 1 images will be returned.\n",
    "        '''\n",
    "        ct_ind, ind = self.train_img_ct_ind[img_id]['ct_ind'], self.train_img_ct_ind[img_id]['ind']\n",
    "        ct = self.train_img_ct[ct_ind]\n",
    "        n = len(ct)\n",
    "        low, high = ind-n_slices, ind+n_slices\n",
    "        if low < 0:\n",
    "            high += abs(low)\n",
    "            low = 0\n",
    "        elif high >= n:\n",
    "            low -= high - (n-1)\n",
    "            high = n-1\n",
    "        return [get_img_tensor(self.train_img_dir+img_id+'.png') for img_id in ct[low:high+1]]\n",
    "\n",
    "    def get_nearby_slices_features(self, img_id, n_slices):\n",
    "        \"\"\"\n",
    "        :img_id (str): the ID of the slice to find nearby slices for\n",
    "        :n_slices (int): number of slices, both above and below, to return.\n",
    "        \n",
    "        :return: list of 2*n_slices + 1 feature vectors arranged in a 2D Tensor\n",
    "        \n",
    "        Gets the feature vectors of nearby slices rather than their raw images. Results in better performance.\n",
    "        \"\"\"\n",
    "        ct_ind, ind = self.train_img_ct_ind[img_id]['ct_ind'], self.train_img_ct_ind[img_id]['ind']\n",
    "        ct = self.train_img_ct[ct_ind]\n",
    "        n = len(ct)\n",
    "        low, high = ind-n_slices, ind+n_slices\n",
    "        if low < 0:\n",
    "            high += abs(low)\n",
    "            low = 0\n",
    "        elif high >= n:\n",
    "            low -= high - (n-1)\n",
    "            high = n-1\n",
    "\n",
    "        res = []\n",
    "        for img_id in ct[low:high+1]:\n",
    "            try:\n",
    "                res.append(np.load(self.feature_dir+img_id+'.npy'))\n",
    "            except:\n",
    "                pass\n",
    "        return tf.squeeze(res)\n",
    "        \n",
    "    def precompute_features(self):\n",
    "        \"\"\"\n",
    "        Passes all the images through the extractor model first before training and save them in self.feature_dir as \n",
    "        feature vectors.\n",
    "        \"\"\"\n",
    "        \n",
    "        to_compute = set()\n",
    "        present = set(x.split('.')[0] for x in os.listdir(self.feature_dir))\n",
    "        print('Collecting necessary slices...')\n",
    "        for img_id in tqdm(self.x):\n",
    "            ct = self.train_img_ct[self.train_img_ct_ind[img_id]['ct_ind']]\n",
    "            to_compute.update(ct)\n",
    "            \n",
    "        print(f'{len(to_compute.intersection(present))} feature vectors already present')\n",
    "        to_compute = list(to_compute.difference(present))\n",
    "        print(f'Computing {len(to_compute)} new feature vectors...')\n",
    "        compute_batch_size = 200\n",
    "        \n",
    "        if len(to_compute) == 0:\n",
    "            return\n",
    "        \n",
    "        for i in tqdm(range(ceil(len(to_compute)//compute_batch_size)+1)):\n",
    "            batch_names = to_compute[i*compute_batch_size : (i+1)*compute_batch_size]\n",
    "            batch = np.array(Parallel(n_jobs=-1, backend='threading')(delayed(get_img_tensor)(self.img_dir+img_id+'.png') for img_id in batch_names))\n",
    "            try:\n",
    "                batch = self.extractor.predict(batch)\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "                print(batch.shape)\n",
    "                sys.exit(0)\n",
    "            for i,feat_vec in enumerate(batch):\n",
    "                np.save(self.feature_dir+batch_names[i]+'.npy', feat_vec)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch_x_names = [self.get_nearby_slices_names(img_id, self.n_slices) for img_id in batch_x]\n",
    "        batch_y = tf.convert_to_tensor([np.array([self.y[img_id] for img_id in slices]).flatten() for slices in batch_x_names])\n",
    "        batch_x = tf.convert_to_tensor([[np.squeeze(np.load(self.feature_dir+img_id+'.npy')) for img_id in slices] for slices in batch_x_names])\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        ind = np.random.choice(list(range(len(os.listdir(self.img_dir)))), size=self.train_cutoff, replace=False)\n",
    "        self.x = [img_name.split('.')[0] for img_name in np.array(os.listdir(self.img_dir))[ind]]\n",
    "        self.precompute_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ee137dc-06c0-492e-809f-d7270d2ea066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNAModel:\n",
    "    def __init__(self, train_img_dir, train_label_path, train_ct_path, train_coord_path, base_model_path, feature_dir):\n",
    "        self.train_img_dir = train_img_dir #directory containing all 3-channel PNG images of scans\n",
    "        self.train_label_path = train_label_path #path to CSV file containing binary encodings of the labels\n",
    "        self.train_ct_path = train_ct_path #path to CSV where each line lists all image IDs in a certain CT scan\n",
    "        self.train_coord_path = train_coord_path #path to CSV where each line contains an image ID and its (x,y,z) coordinates in its CT \n",
    "        self.base_model_path = base_model_path #path to the base CNN model which will be used to extract features \n",
    "        self.feature_dir = feature_dir #directory containing .npy files which are feature vectors of each image. The directory may be incomplete;\n",
    "                                        #if so, this model will automatically generate feature vectors as needed\n",
    "    \n",
    "        self.train_img_ct = None\n",
    "        self.train_img_ct_ind = None\n",
    "        self.labels = None\n",
    "        self.extractor = None\n",
    "        self.model = None\n",
    "        self.callbacks = None\n",
    "    \n",
    "        self.assemble_ct_scans()\n",
    "        self.retrieve_labels()\n",
    "        self.retrieve_feature_extractor()\n",
    "        \n",
    "    def assemble_ct_scans(self):\n",
    "        \"\"\"\n",
    "        Collects image IDs from the same CT scan and stores them in a dictionary in sorted order (increasing z-value)\n",
    "        \"\"\"\n",
    "        self.train_img_ct = {} # scan index : list of image IDs in the scan\n",
    "        self.train_img_ct_ind = {} #image ID : {\"ct_ind\": index of the CT scan this image belongs to (key in train_img_ct), \"ind\": index in the list of slices}\n",
    "        i = 0\n",
    "        \n",
    "        train_img_coords = pd.read_csv(self.train_coord_path, index_col=0, names=['x','y','z'])\n",
    "        def populate_ct_info(row,i):\n",
    "            #takes in a list of Image IDs of slices in a CT scan\n",
    "            row = row[1:]\n",
    "            row.sort(key=lambda x: train_img_coords.loc[x]['z'])\n",
    "            for slice_ind, img_id in enumerate(row):\n",
    "                self.train_img_ct_ind[img_id] = {'ct_ind': i, 'ind': slice_ind}\n",
    "            self.train_img_ct[i] = row\n",
    "\n",
    "        \n",
    "        with open(self.train_ct_path) as scans:\n",
    "            reader = csv.reader(scans, delimiter=',')\n",
    "            Parallel(n_jobs=-1, backend='threading', require='sharedmem', batch_size=75)(delayed(populate_ct_info)(row,i) for i, row in tqdm(list(enumerate(list(reader)))))\n",
    "            \n",
    "    def retrieve_labels(self):\n",
    "        \"\"\"\n",
    "        Retrieves labels from the train_label_path and stores them in a DataFrame\n",
    "        \"\"\"\n",
    "        labels = pd.read_csv(self.train_label_path)\n",
    "        self.labels = {l[0]: l[1:].astype(np.int8) for l in labels.to_numpy()}\n",
    "        \n",
    "    def retrieve_feature_extractor(self):\n",
    "        \"\"\"\n",
    "        Retrieves the model to be used for feature extraction\n",
    "        \"\"\"\n",
    "        base_model = keras.models.load_model(self.base_model_path)\n",
    "        self.extractor = keras.models.Sequential(base_model.layers[:-1])\n",
    "    \n",
    "    def initialize_model(self, saved_model_path=None):\n",
    "        \"\"\"\n",
    "        :saved_model_path (str): The directory containing files for the saved model, including 'saved_model.pb'. If None,\n",
    "        this method will create a new model.\n",
    "        \n",
    "        Initialize the RNN model for training, either by creating a new one or using a previously\n",
    "        saved model path.\n",
    "        \"\"\"\n",
    "        if not saved_model_path: \n",
    "            #create a new model if no checkpoint was provided\n",
    "            self.model = keras.Sequential([Bidirectional(LSTM(512, return_sequences=True, name='lstm0')),\n",
    "                              Bidirectional(LSTM(512, return_sequences=True, name='lstm1')),\n",
    "                              Bidirectional(GRU(256, return_sequences=True, name='gru0')),\n",
    "                              Conv1D(6, 1, padding='same', activation='sigmoid'),\n",
    "                              Flatten()\n",
    "                             ])\n",
    "            print('Created new model')\n",
    "        else:\n",
    "            self.model = keras.models.load_model(saved_model_path)\n",
    "            print(f'Found model at {saved_model_path}')\n",
    "        \n",
    "        self.model.build(input_shape=(None, 19,2048))\n",
    "        self.model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "              metrics=['binary_accuracy', \n",
    "                       keras.metrics.AUC(multi_label=True, num_labels=114, from_logits=False),\n",
    "                       keras.metrics.Precision(), keras.metrics.Recall()],\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=3e-5))\n",
    "        \n",
    "    \n",
    "    def train(self, batch_size=32, n_batches=2000, n_slices=9, n_epochs=2, save_path=None):\n",
    "        if not self.model:\n",
    "            print(\"ERROR: please call self.initialize_model first before training\")\n",
    "            return\n",
    "        \n",
    "        train_cutoff = batch_size*n_batches\n",
    "        train_sequence = RSNASequence(self.labels, train_cutoff, batch_size, self.extractor, self.train_img_dir, n_slices, \\\n",
    "                                     self.train_img_ct, self.train_img_ct_ind, self.feature_dir)\n",
    "        if save_path:\n",
    "            self.callbacks = [keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)]\n",
    "        \n",
    "        self.model.fit(x=train_sequence, epochs=n_epochs, callbacks=self.callbacks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b2b066-b8cc-4e55-8d4d-bc3a8dc6a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18931/18931 [00:46<00:00, 410.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model = RSNAModel(train_img_dir, train_label_path, train_ct_path, train_coord_path, base_model_path, feature_dir='extracted_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aeae402-f9ac-4f27-87de-2928a9216f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model at experiment-3-checkpoints/checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.initialize_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437afcf4-9e96-4e89-8a53-4cd399aceb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(save_path=saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af298d70",
   "metadata": {},
   "source": [
    "After training, another python file would be used to load the trained model and handle API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e1569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
