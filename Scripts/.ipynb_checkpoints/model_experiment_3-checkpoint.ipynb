{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb7d6f6-a430-4983-9b79-5808a2f74df4",
   "metadata": {},
   "source": [
    "This notebook is for experimenting with a CNN to PCA to RNN LSTM network model. The CNN will first have its dimensions reduced by PCA, then the LSTM network train on one CT scan at a time; it will take in the principal components of each slice in a scan and output predictions for ICH on each slice, while taking into account spacial dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b43a76-420e-4b9d-9338-6011a89bcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dcm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035b7089-3fa6-4796-b898-8a8070d777e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: A100-SXM4-40GB, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "train_img_dir = 'rsna-intracranial-hemorrhage-detection/stage_2_train_imgs/'\n",
    "train_label_path = 'rsna-intracranial-hemorrhage-detection/train_labels.csv'\n",
    "train_ct_path = 'rsna-intracranial-hemorrhage-detection/train_ct_scans.csv'\n",
    "train_coord_path = 'rsna-intracranial-hemorrhage-detection/train_ct_coords.csv'\n",
    "model_path = '/home/jupyter/base-cnn-model/checkpoint.ckpt'\n",
    "\n",
    "test_img_dir = 'rsna-intracranial-hemorrhage-detection/stage_2_test_imgs/'\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8fcc90-6650-4f22-b775-56c47e9969cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18931/18931 [00:46<00:00, 406.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_img_ct = {} # scan index : list of image IDs in the scan\n",
    "train_img_ct_ind = {} #image ID : {\"ct_ind\": index of the CT scan this image belongs to (key in train_img_ct), \"ind\": index in the list of slices}\n",
    "i = 0\n",
    "\n",
    "def populate_ct_info(row,i):\n",
    "    #takes in a list of Image IDs of slices in a CT scan\n",
    "    row = row[1:]\n",
    "    row.sort(key=lambda x: train_img_coords.loc[x]['z'])\n",
    "    for slice_ind, img_id in enumerate(row):\n",
    "        train_img_ct_ind[img_id] = {'ct_ind': i, 'ind': slice_ind}\n",
    "    train_img_ct[i] = row\n",
    "\n",
    "train_img_coords = pd.read_csv(train_coord_path, index_col=0, names=['x','y','z'])\n",
    "with open(train_ct_path) as scans:\n",
    "    reader = csv.reader(scans, delimiter=',')\n",
    "    Parallel(n_jobs=-1, backend='threading', require='sharedmem', batch_size=75)(delayed(populate_ct_info)(row,i) for i, row in tqdm(list(enumerate(list(reader)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad88a61-4f4d-4f23-a258-371ecce50c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(train_label_path)\n",
    "labels = {l[0]: l[1:].astype(np.int8) for l in labels.to_numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2255ca3f-7734-4201-9036-65a66f8c0380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101 (Functional)      (None, 16, 16, 2048)      42658176  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,658,176\n",
      "Trainable params: 42,552,832\n",
      "Non-trainable params: 105,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.models.load_model(model_path)\n",
    "extractor = keras.models.Sequential(base_model.layers[:-1])\n",
    "extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7879099d-102e-43e1-aaf9-7e4d802df244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearby_slices(img_dir, img_id, n_slices):\n",
    "    '''\n",
    "    will retrieve n_slices slices from BOTH above and below the given image. If there is not enough space, it will\n",
    "    add more slices either below (if the image is near the top of the scan) or above (if the image is near the bottom of the scan).\n",
    "    Exactly 2*n_slices + 1 images will be returned\n",
    "    '''\n",
    "    ct_ind, ind = train_img_ct_ind[img_id]['ct_ind'], train_img_ct_ind[img_id]['ind']\n",
    "    ct = train_img_ct[ct_ind]\n",
    "    n = len(ct)\n",
    "    low, high = ind-n_slices, ind+n_slices\n",
    "    if low < 0:\n",
    "        high += abs(low)\n",
    "        low = 0\n",
    "    elif high >= n:\n",
    "        low -= high - (n-1)\n",
    "        high = n-1\n",
    "    # return Parallel(n_jobs=-3)(delayed(get_img_tensor)(img_dir+img_id+'.png') for img_id in ct[low:high+1])\n",
    "    return [get_img_tensor(img_dir+img_id+'.png') for img_id in ct[low:high+1]]\n",
    "\n",
    "def get_nearby_slices_features(feature_dir, img_id, n_slices):\n",
    "    ct_ind, ind = train_img_ct_ind[img_id]['ct_ind'], train_img_ct_ind[img_id]['ind']\n",
    "    ct = train_img_ct[ct_ind]\n",
    "    n = len(ct)\n",
    "    low, high = ind-n_slices, ind+n_slices\n",
    "    if low < 0:\n",
    "        high += abs(low)\n",
    "        low = 0\n",
    "    elif high >= n:\n",
    "        low -= high - (n-1)\n",
    "        high = n-1\n",
    "    # return Parallel(n_jobs=-3)(delayed(get_img_tensor)(img_dir+img_id+'.png') for img_id in ct[low:high+1])\n",
    "    res = []\n",
    "    for img_id in ct[low:high+1]:\n",
    "        try:\n",
    "            res.append(np.load(feature_dir+img_id+'.npy'))\n",
    "        except:\n",
    "            pass\n",
    "    return tf.squeeze(res)\n",
    "\n",
    "def get_nearby_slices_names(img_id, n_slices):\n",
    "    ct_ind, ind = train_img_ct_ind[img_id]['ct_ind'], train_img_ct_ind[img_id]['ind']\n",
    "    ct = train_img_ct[ct_ind]\n",
    "    n = len(ct)\n",
    "    low, high = ind-n_slices, ind+n_slices\n",
    "    if low < 0:\n",
    "        high += abs(low)\n",
    "        low = 0\n",
    "    elif high >= n:\n",
    "        low -= high - (n-1)\n",
    "        high = n-1\n",
    "    return ct[low:high+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de69106-63ef-4727-bf41-512491f2f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTSequence(keras.utils.Sequence):\n",
    "    def __init__(self, image_lists, labels, batch_size, cnn_model, image_dir, feature_dir='extracted_features/'):\n",
    "        self.x = image_lists #list of lists/arrays; each inner list is contains IDs of images in a scan\n",
    "        self.y = labels #dict of image ID (str) : of length 6 arrays. Each array represents binary labels for one slice\n",
    "        self.batch_size = batch_size\n",
    "        self.model = cnn_model #model to pass each slice through; it should return a feature vector for each slice\n",
    "        self.image_dir = image_dir #directory that holds the images\n",
    "        self.feature_dir = feature_dir\n",
    "        \n",
    "        self.precompute_features()\n",
    "    \n",
    "    def precompute_features(self):\n",
    "        #pass all the images through the extractor model first before training and save them\n",
    "        for ct in tqdm(self.x):\n",
    "            for img_id in tqdm(ct):\n",
    "                x = get_img_tensor(self.image_dir+img_id+'.png')\n",
    "                print(x.shape)\n",
    "                x = np.array(self.model.predict(tf.expand_dims(x, axis=0)))\n",
    "                np.save(self.feature_dir+img_id+'.npy', x)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = [ [self.y[img_id] for img_id in ct] for ct in batch_x]\n",
    "        print(len(batch_x), len(batch_y))\n",
    "        batch_x = [ tf.convert_to_tensor([get_img_tensor(self.image_dir + img_id + '.png') for img_id in ct]) for ct in batch_x]\n",
    "        print([x.shape for x in batch_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad85af8-e4e9-47e9-8378-f14397369a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_tensor(img_path):\n",
    "    return tf.convert_to_tensor(np.asarray(Image.open(img_path), dtype=np.float32) / 255.)\n",
    "\n",
    "class RSNASequence2(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, cnn_model, img_dir, n_slices, feature_dir='extracted_features/'):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.img_dir = img_dir\n",
    "        self.n_slices = n_slices\n",
    "        self.model = cnn_model\n",
    "        self.feature_dir = feature_dir\n",
    "        \n",
    "        self.precompute_features()\n",
    "            \n",
    "        \n",
    "    def precompute_features(self):\n",
    "        #pass all the images through the extractor model first before training and save them\n",
    "        to_compute = set()\n",
    "        present = set(x.split('.')[0] for x in os.listdir(self.feature_dir))\n",
    "        print('Collecting necessary slices...')\n",
    "        for img_id in tqdm(self.x):\n",
    "            ct = train_img_ct[train_img_ct_ind[img_id]['ct_ind']]\n",
    "            to_compute.update(ct)\n",
    "            \n",
    "        print(f'{len(to_compute.intersection(present))} feature vectors already present')\n",
    "        to_compute = list(to_compute.difference(present))\n",
    "        print(f'Computing {len(to_compute)} new feature vectors...')\n",
    "        compute_batch_size = 200\n",
    "        \n",
    "        if len(to_compute) == 0:\n",
    "            return\n",
    "        \n",
    "        for i in tqdm(range(ceil(len(to_compute)//compute_batch_size)+1)):\n",
    "            batch_names = to_compute[i*compute_batch_size : (i+1)*compute_batch_size]\n",
    "            batch = np.array(Parallel(n_jobs=-1, backend='threading')(delayed(get_img_tensor)(self.img_dir+img_id+'.png') for img_id in batch_names))\n",
    "            try:\n",
    "                batch = self.model.predict(batch)\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "                print(batch.shape)\n",
    "                sys.exit(0)\n",
    "            for i,feat_vec in enumerate(batch):\n",
    "                np.save(self.feature_dir+batch_names[i]+'.npy', feat_vec)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch_x_names = [get_nearby_slices_names(img_id, self.n_slices) for img_id in batch_x]\n",
    "        batch_y = tf.convert_to_tensor([np.array([self.y[img_id] for img_id in slices]).flatten() for slices in batch_x_names])\n",
    "        batch_x = tf.convert_to_tensor([[np.squeeze(np.load(self.feature_dir+img_id+'.npy')) for img_id in slices] for slices in batch_x_names])\n",
    "        return batch_x, batch_y\n",
    "        # batch_x = [tf.convert_to_tensor(\n",
    "                # get_nearby_slices_features(self.feature_dir, img_id, self.n_slices)) for img_id in batch_x]\n",
    "        # return tf.convert_to_tensor(batch_x), tf.convert_to_tensor(batch_y)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        ind = np.random.choice(list(range(len(os.listdir(train_img_dir)))), size=train_cutoff, replace=False)\n",
    "        self.x = [img_name.split('.')[0] for img_name in np.array(os.listdir(train_img_dir))[ind]]\n",
    "        # os.system('rm ' + self.feature_dir + '*')\n",
    "        self.precompute_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a36f919-2207-4fa5-ae1d-94f3b692cd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting necessary slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [00:00<00:00, 233014.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724124 feature vectors already present\n",
      "Computing 0 new feature vectors...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_batches = 2000\n",
    "n_slices = 9\n",
    "\n",
    "# train_cutoff = batch_size*n_batches\n",
    "# ind = np.random.choice(list(range(len(train_img_ct))), size=train_cutoff, replace=False)\n",
    "# cts = list(train_img_ct.values())\n",
    "# x = [cts[i] for i in ind]\n",
    "# train = CTSequence(x, labels, batch_size, extractor, train_img_dir)\n",
    "\n",
    "train_cutoff = batch_size*n_batches #training the whole dataset takes ~9 hours, so we cut it short for proof-of-concept purposes.\n",
    "ind = np.random.choice(list(range(len(os.listdir(train_img_dir)))), size=train_cutoff, replace=False)\n",
    "\n",
    "x = [img_name.split('.')[0] for img_name in np.array(os.listdir(train_img_dir))[ind]]\n",
    "train = RSNASequence2(x, labels, batch_size, extractor, train_img_dir, n_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65d49bec-5488-481e-ae0e-406ef94dca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2.256608009338379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 19, 2048]), TensorShape([32, 114]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "print(len(train))\n",
    "x, y = train[0]\n",
    "print(time.time()-t)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79b07813-4cb4-4969-8e14-e3c213353635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_model = keras.Sequential([Bidirectional(LSTM(512, return_sequences=True, name='lstm0')),\n",
    "#                               Bidirectional(LSTM(512, return_sequences=True, name='lstm1')),\n",
    "#                               Bidirectional(GRU(256, return_sequences=True, name='gru0')),\n",
    "#                               Conv1D(6, 1, padding='same', activation='sigmoid'),\n",
    "#                               Flatten()\n",
    "#                              ])\n",
    "\n",
    "rnn_model = keras.models.load_model('experiment-3-checkpoints/checkpoint.ckpt')\n",
    "\n",
    "rnn_model.build(input_shape=(None, 19,2048))\n",
    "\n",
    "rnn_model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "              metrics=['binary_accuracy', \n",
    "                       keras.metrics.AUC(multi_label=True, num_labels=114, from_logits=False),\n",
    "                       keras.metrics.Precision(), keras.metrics.Recall()],\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=3e-5))\n",
    "\n",
    "# rnn_model.load_weights('experiment-3-checkpoints/checkpoint.ckpt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c4125f6-24b9-4b68-bdb3-38c8373cc0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(filepath='experiment-3-checkpoints/checkpoint.ckpt',\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e851bb-070d-4a0e-bc00-314f00242efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_66 (Bidirecti  (None, None, 1024)       10489856  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_67 (Bidirecti  (None, None, 1024)       6295552   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_68 (Bidirecti  (None, None, 512)        1969152   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " conv1d_48 (Conv1D)          (None, None, 6)           3078      \n",
      "                                                                 \n",
      " flatten_23 (Flatten)        (None, None)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,757,638\n",
      "Trainable params: 18,757,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a595aa5b-1af8-471c-aff6-6830f3ba4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0420 - binary_accuracy: 0.9843 - auc_2: 0.9905 - precision_2: 0.9011 - recall_2: 0.8619\n",
      "Epoch 00001: saving model to experiment-3-checkpoints/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiment-3-checkpoints/checkpoint.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiment-3-checkpoints/checkpoint.ckpt/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c939e490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c93439d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c930c990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c9314390> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f08c931fe10> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f08c932a690> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 686s 343ms/step - loss: 0.0420 - binary_accuracy: 0.9843 - auc_2: 0.9905 - precision_2: 0.9011 - recall_2: 0.8619\n",
      "Collecting necessary slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [00:00<00:00, 240951.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723289 feature vectors already present\n",
      "Computing 0 new feature vectors...\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0420 - binary_accuracy: 0.9844 - auc_2: 0.9905 - precision_2: 0.9031 - recall_2: 0.8619\n",
      "Epoch 00002: saving model to experiment-3-checkpoints/checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn, lstm_cell_14_layer_call_and_return_conditional_losses, lstm_cell_16_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiment-3-checkpoints/checkpoint.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: experiment-3-checkpoints/checkpoint.ckpt/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c939e490> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c93439d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c930c990> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f08c9314390> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f08c931fe10> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f08c932a690> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 674s 337ms/step - loss: 0.0420 - binary_accuracy: 0.9844 - auc_2: 0.9905 - precision_2: 0.9031 - recall_2: 0.8619\n",
      "Collecting necessary slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [00:00<00:00, 242131.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723692 feature vectors already present\n",
      "Computing 0 new feature vectors...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f092f785bd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(x=train, epochs=2, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11e302f6-4484-4016-badd-d2e479426b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save('experiment-3-checkpoints/rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b37454-1f21-4dab-8a50-f361d826aa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting necessary slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [00:00<00:00, 239140.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722953 feature vectors already present\n",
      "Computing 0 new feature vectors...\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0418 - binary_accuracy: 0.9844 - auc_2: 0.9910 - precision_2: 0.8931 - recall_2: 0.8729Collecting necessary slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [00:00<00:00, 229739.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723738 feature vectors already present\n",
      "Computing 0 new feature vectors...\n",
      "2000/2000 [==============================] - 629s 313ms/step - loss: 0.0418 - binary_accuracy: 0.9844 - auc_2: 0.9910 - precision_2: 0.8931 - recall_2: 0.8729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.041804730892181396,\n",
       " 0.9843528270721436,\n",
       " 0.9909812808036804,\n",
       " 0.8930655717849731,\n",
       " 0.8729138374328613]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.on_epoch_end()\n",
    "rnn_model.evaluate(x=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee1ec0-a4fc-4e2c-9d90-ce60d3e1a8da",
   "metadata": {},
   "source": [
    "This model takes as input the length 2048 vectors obtained from the penultimate layer of the tuned CNN model (from Model Experiment 1). For a given training image, the model selects 9 images above and below the image from its corresponding CT scan (if there are not enough images below/above, the model compensates by gathering more images above/below in order to always return 19 images; it is guaranteed that each CT scan contains at least 19 images). Then, these 19 images are passed through the CNN to obtain 19 vectors of length 2048, arranged in a 19x2048 matrix and interpreted as time series data. The RNN contains bidirectional LSTM and GRU layers which allows it to consider features both before and after a given time step (i.e., it considers all surrounding image slices). The RNN then outputs ICH predicts for all 19 slices. It is trained with the same loss (binary cross entropy) and evaluation metrics as Model Experiment 1.\n",
    "\n",
    "This model performs much better than a simple CNN or dual CNN, as it considers nearby slices as a time series input and is able to get information from nearby slices to locate ICH. As it is trained on the feature vectors from the initial single CNN, model performance would likely be improve if the original CNN was trained for longer.\n",
    "\n",
    "However, performance is very good compared to the minimal effort ResNet101-based CNN. The precision of ~89-90% and recall of ~86-87% are much better than the CNN's precision of ~85% and recall of ~70%. This (likely) indicates that using the surrounding temporal slices of the input and feeding them to the Bidirectional RNN allows the model to combine information from nearby slices to make a much more accurate classification. Additionally, we see that the AUC is ~0.99 compared to the CNN's AUC of ~0.96. As the CNN's AUC converged quickly during training and barely moved, this is telling us that the model was making mostly negative (predicting no ICH) predictions and was decent at best at predicting positive classes. However, as the AUC of the RNN model rose significantly, this means that the RNN model is much much better at predicting the positive classes, as desired. Also, we see that the loss of ~0.04 is lower compared to the CNN's loss of ~0.065, meaning that the RNN model is also much more confident (most predictions are very close to 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270ef85-e976-48b5-8c8b-73a01d0d9bbe",
   "metadata": {},
   "source": [
    "Note: Model Experiment 2 is not included because it was a massive failure. The experiment attempted to use the same type of data as Model Experiment 3, except it passed that data into another CNN instead of an RNN; however both precision and recall remained below 10% for most of training which is absolutely abysmal. It's safe to say that a CNN is not sufficient to analyze temporal data such as this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737778b3-40d1-4529-b4c5-7c22fa9747a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
